{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:03:07.316025Z",
     "start_time": "2020-09-01T13:03:06.726055Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算图\n",
    "## 计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:03:07.524716Z",
     "start_time": "2020-09-01T13:03:07.317993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.])\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([1.], requires_grad=True)\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "\n",
    "a = torch.add(w, x)\n",
    "b = torch.add(w, 1)\n",
    "y = torch.mul(a, b)\n",
    "\n",
    "y.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 叶子节点这个属性(还记得张量的属性里面有一个 is_leaf 吗）: **叶子节点：用户创建的节点**， 比如上面的 x 和 w。叶子节点是非常关键的，在上面的正向计算和反向计算中，其实都是依赖于我们叶子节点进行计算的。is_leaf: 指示张量是否是叶子节点。\n",
    "\n",
    "为什么要设置叶子节点的这个概念的？主要是为了节省内存，因为我们在反向传播完了之后，非叶子节点的梯度是默认被释放掉的。我们可以根据上面的那个计算过程，来看看 w，x, a, b, y 的 is_leaf 属性，和它们各自的梯度情况："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:03:07.653735Z",
     "start_time": "2020-09-01T13:03:07.525674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_leaf:\n",
      " True True False False False\n",
      "gradient:\n",
      " tensor([5.]) tensor([2.]) None None None\n"
     ]
    }
   ],
   "source": [
    "# 查看叶子结点\n",
    "print(\"is_leaf:\\n\", w.is_leaf, x.is_leaf, a.is_leaf, b.is_leaf, y.is_leaf)\n",
    "# 查看梯度， 默认是只保留叶子节点的梯度的\n",
    "print(\"gradient:\\n\", w.grad, x.grad, a.grad, b.grad, y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:03:07.897589Z",
     "start_time": "2020-09-01T13:03:07.654732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient:\n",
      " tensor([5.]) tensor([2.]) tensor([2.]) None None\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([1.], requires_grad=True)\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "\n",
    "a = torch.add(w, x)\n",
    "a.retain_grad()\n",
    "b = torch.add(w, 1)\n",
    "y = torch.mul(a, b)\n",
    "\n",
    "y.backward()\n",
    "#查看梯度， 默认是只保留叶子节点的梯度的\n",
    "print(\"gradient:\\n\", w.grad, x.grad, a.grad, b.grad, y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. grad_fn：记录创建该张量时所用的方法（函数），记录这个方法主要「用于梯度的求导」。要不然怎么知道具体是啥运算？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:03:08.168709Z",
     "start_time": "2020-09-01T13:03:07.898552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_fn:\n",
      " None None <AddBackward0 object at 0x000001AB1EA90E10> <AddBackward0 object at 0x000001AB1EA90EB8> <MulBackward0 object at 0x000001AB1EA90F60>\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([1.], requires_grad=True)\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "\n",
    "a = torch.add(w, x)\n",
    "a.retain_grad()\n",
    "b = torch.add(w, 1)\n",
    "y = torch.mul(a, b)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "# 查看 grad_fn   这个表示怎么得到的\n",
    "print(\"grad_fn:\\n\", w.grad_fn, x.grad_fn, a.grad_fn, b.grad_fn, y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 动态图\n",
    "根据计算图的搭建方式，可以将计算图分为动态图和静态图。\n",
    "- 静态图：先搭建图，后运算。高效，不灵活（TensorFlow）\n",
    "- 动态图：运算与搭建同时进行。灵活，易调节（Pytorch）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:03:08.457796Z",
     "start_time": "2020-09-01T13:03:08.170668Z"
    }
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# # 声明两个常量\n",
    "# w = tf.constant(1.)\n",
    "# x = tf.constant(2.)\n",
    "\n",
    "# # 搭建静态图\n",
    "# a = tf.add(w, x)\n",
    "# b = tf.add(w, 1)\n",
    "# y = tf.multiply(a, b)\n",
    "\n",
    "# # 这时候还没开始计算\n",
    "# print(y)   # Tensor(\"Mul_4:0\", shape=(), dtype=float32)， 只是计算图的一个节点\n",
    "\n",
    "# with tf.Session() as sess:  \n",
    "#     print(sess.run(y))   # 这里才开始进行计算， 6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T13:03:08.698366Z",
     "start_time": "2020-09-01T13:03:08.463744Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([1.], requires_grad=True)\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "\n",
    "a = torch.add(w, x)\n",
    "b = torch.add(w, 1)\n",
    "y = torch.mul(a, b)\n",
    "print(y)    # tensor([6.], grad_fn=<MulBackward0>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自动求导机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
