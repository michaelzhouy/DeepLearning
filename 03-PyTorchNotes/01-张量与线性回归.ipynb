{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T08:56:18.725868Z",
     "start_time": "2020-09-01T08:56:18.162308Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 张量的简介与创建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 直接创建张量「torch.Tensor()：功能：从 data 创建 Tensor」"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T08:56:18.735811Z",
     "start_time": "2020-09-01T08:56:18.727872Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.ones((3, 3))\n",
    "arr.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T08:56:20.596077Z",
     "start_time": "2020-09-01T08:56:18.737360Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor(arr, device='cuda')\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过 numpy 数组来创建「torch.from_numpy(ndarry)：从 numpy 创建 tensor」注意：这个创建的 Tensor 与原 ndarray 「共享内存」, 当修改其中一个数据的时候，另一个也会被改动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T08:56:20.606525Z",
     "start_time": "2020-09-01T08:56:20.597074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]] \n",
      " tensor([[1, 2, 3],\n",
      "        [4, 5, 6]], dtype=torch.int32)\n",
      "**********\n",
      "[[0 2 3]\n",
      " [4 5 6]] \n",
      " tensor([[0, 2, 3],\n",
      "        [4, 5, 6]], dtype=torch.int32)\n",
      "**********\n",
      "[[  0   2   3]\n",
      " [  4 100   6]] \n",
      " tensor([[  0,   2,   3],\n",
      "        [  4, 100,   6]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "t = torch.from_numpy(arr)\n",
    "\n",
    "print(arr, '\\n', t)\n",
    "t[0, 0] = 0\n",
    "print('*' * 10)\n",
    "\n",
    "print(arr, '\\n', t)\n",
    "t[1, 1] = 100\n",
    "print('*' * 10)\n",
    "\n",
    "print(arr, '\\n', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 依据数值创建「torch.zeros()：依 size 创建全 0 的张量」"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T08:56:20.623937Z",
     "start_time": "2020-09-01T08:56:20.606525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]]) \n",
      " tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "2161625244008 2161625244008 True\n"
     ]
    }
   ],
   "source": [
    "out_t = torch.tensor([1])\n",
    "t = torch.zeros((3, 3), out=out_t)\n",
    "\n",
    "print(out_t, '\\n', t)\n",
    "print(id(t), id(out_t), id(t) == id(out_t))   # 这个看内存地址"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T08:56:20.630922Z",
     "start_time": "2020-09-01T08:56:20.626929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.zeros_like(out_t)   # 这里的input要是个张量\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T08:56:20.638896Z",
     "start_time": "2020-09-01T08:56:20.632915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10, 10, 10],\n",
       "        [10, 10, 10],\n",
       "        [10, 10, 10]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.full((3,3), 10, out=t)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T08:56:20.646875Z",
     "start_time": "2020-09-01T08:56:20.639894Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 4, 6, 8])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(2, 10, 2)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T08:56:20.658844Z",
     "start_time": "2020-09-01T08:56:20.648871Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.,  4.,  6.,  8., 10.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.linspace(2, 10, 5)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T08:56:20.670813Z",
     "start_time": "2020-09-01T08:56:20.659841Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.0000,  3.6000,  5.2000,  6.8000,  8.4000, 10.0000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.linspace(2, 10, 6)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T08:56:20.681802Z",
     "start_time": "2020-09-01T08:56:20.672808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.3975, -1.0220, -1.4656, -0.4170])\n"
     ]
    }
   ],
   "source": [
    "# 第一种模式 - 均值是标量， 方差是标量 - 此时产生的是一个分布， 从这一个分布中抽样相应的个数，所以这个必须指定size，也就是抽取多少个数\n",
    "t_normal = torch.normal(0, 1, size=(4,))\n",
    "print(t_normal)     # 来自同一个分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T08:56:20.694748Z",
     "start_time": "2020-09-01T08:56:20.682780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([ 1.4128,  2.2744,  3.1083, -5.7320])\n"
     ]
    }
   ],
   "source": [
    "# 第二种模式 - 均值是标量， 方差是张量 - 此时会根据方差的形状大小，产生同样多个分布，每一个分布的均值都是那个标量\n",
    "std = torch.arange(1, 5, dtype=torch.float)\n",
    "print(std.dtype)\n",
    "t_normal2 = torch.normal(1, std)\n",
    "print(t_normal2)        # 也产生来四个数，但是这四个数分别来自四个不同的正态分布，这些分布均值相等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T08:56:20.704721Z",
     "start_time": "2020-09-01T08:56:20.696756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.3912, 4.0056, 4.1198, 6.2855])\n"
     ]
    }
   ],
   "source": [
    "# 第三种模式 - 均值是张量，方差是标量 - 此时也会根据均值的形状大小，产生同样多个方差相同的分布，从这几个分布中分别取一个值作为结果\n",
    "mean = torch.arange(1, 5, dtype=torch.float)\n",
    "t_normal3 = torch.normal(mean, 1)\n",
    "print(t_normal3)     # 来自不同的分布，但分布里面方差相等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T08:56:20.715692Z",
     "start_time": "2020-09-01T08:56:20.705718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.3332,  3.1335, -0.4978,  2.9007])\n"
     ]
    }
   ],
   "source": [
    "# 第四种模式 - 均值是张量， 方差是张量 - 此时需要均值的个数和方差的个数一样多，分别产生这么多个正太分布，从这里面抽取一个值\n",
    "mean = torch.arange(1, 5, dtype=torch.float)\n",
    "std = torch.arange(1, 5, dtype=torch.float)\n",
    "t_normal4 = torch.normal(mean, std)\n",
    "print(t_normal4)          # 来自不同的分布，各自有自己的均值和方差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 张量的操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量的基本操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 张量的拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T08:56:20.726693Z",
     "start_time": "2020-09-01T08:56:20.716690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "----------\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      " torch.Size([4, 3])\n",
      "----------\n",
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]]) \n",
      " torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "# 张量的拼接\n",
    "t = torch.ones((2, 3))\n",
    "print(t)\n",
    "print('-' * 10)\n",
    "\n",
    "t_0 = torch.cat([t, t], dim=0)       # 行拼接\n",
    "t_1 = torch.cat([t, t], dim=1)    # 列拼接\n",
    "print(t_0, '\\n', t_0.shape)\n",
    "print('-' * 10)\n",
    "print(t_1, '\\n', t_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T08:56:20.738630Z",
     "start_time": "2020-09-01T08:56:20.727660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n",
      "----------\n",
      "torch.Size([3, 2, 3])\n",
      "----------\n",
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n",
      "----------\n",
      "torch.Size([2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "t_stack = torch.stack([t, t, t], dim=0)\n",
    "print(t_stack)\n",
    "print('-' * 10)\n",
    "print(t_stack.shape)\n",
    "print('-' * 10)\n",
    "\n",
    "t_stack1 = torch.stack([t, t, t], dim=1)\n",
    "print(t_stack1)\n",
    "print('-' * 10)\n",
    "print(t_stack1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 张量的切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T08:56:20.750640Z",
     "start_time": "2020-09-01T08:56:20.739628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]), tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]), tensor([[1.],\n",
      "        [1.]]))\n",
      "第1个张量：tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]), shape is torch.Size([2, 3])\n",
      "第2个张量：tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]), shape is torch.Size([2, 3])\n",
      "第3个张量：tensor([[1.],\n",
      "        [1.]]), shape is torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((2, 7))  # 7\n",
    "# 第一个维度切成三块， 那么应该是(2,3), (2,3), (2,1)  因为7不能整除3，所以每一份应该向上取整，最后不够的有多少算多少\n",
    "list_of_tensors = torch.chunk(a, dim=1, chunks=3)\n",
    "print(list_of_tensors)\n",
    "for idx, t in enumerate(list_of_tensors):\n",
    "    print(\"第{}个张量：{}, shape is {}\".format(idx+1, t, t.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T08:56:20.760622Z",
     "start_time": "2020-09-01T08:56:20.753630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个张量：tensor([[1., 1.],\n",
      "        [1., 1.]]), shape is torch.Size([2, 2])\n",
      "第2个张量：tensor([[1.],\n",
      "        [1.]]), shape is torch.Size([2, 1])\n",
      "第3个张量：tensor([[1., 1.],\n",
      "        [1., 1.]]), shape is torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# split\n",
    "t = torch.ones((2, 5))\n",
    "\n",
    "# [2 , 1, 2]， 这个要保证这个list的大小正好是那个维度的总大小，这样才能切\n",
    "list_of_tensors = torch.split(t, [2, 1, 2], dim=1)\n",
    "for idx, t in enumerate(list_of_tensors):\n",
    "    print(\"第{}个张量：{}, shape is {}\".format(idx+1, t, t.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 张量的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T08:56:20.770594Z",
     "start_time": "2020-09-01T08:56:20.763563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 3, 0],\n",
      "        [0, 2, 3],\n",
      "        [2, 3, 7]])\n",
      "tensor([[0, 0],\n",
      "        [0, 3],\n",
      "        [2, 7]])\n"
     ]
    }
   ],
   "source": [
    "# 从0-8随机产生数组成3*3的矩阵\n",
    "t = torch.randint(0, 9, size=(3, 3))\n",
    "print(t)\n",
    "idx = torch.tensor([0, 2], dtype=torch.long)   # 这里的类型注意一下，要是long类型\n",
    "t_select = torch.index_select(t, dim=1, index=idx)  # 第0列和第2列拼接返回\n",
    "print(t_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T08:56:20.781517Z",
     "start_time": "2020-09-01T08:56:20.772550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask：\n",
      " tensor([[False, False, False],\n",
      "        [False, False, False],\n",
      "        [False, False,  True]])\n",
      "tensor([7])\n"
     ]
    }
   ],
   "source": [
    "mask = t.ge(5)   # le表示<=5, ge表示>=5 gt >5  lt <5\n",
    "print(\"mask：\\n\", mask)\n",
    "\n",
    "# 选出t中大于5的元素\n",
    "t_select1 = torch.masked_select(t, mask)\n",
    "print(t_select1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 张量的变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T08:56:20.791527Z",
     "start_time": "2020-09-01T08:56:20.783510Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 7, 5, 3, 1, 0, 4, 2])\n",
      "t:tensor([6, 7, 5, 3, 1, 0, 4, 2])\n",
      "t_reshape:\n",
      "tensor([[[6, 7],\n",
      "         [5, 3]],\n",
      "\n",
      "        [[1, 0],\n",
      "         [4, 2]]])\n",
      "t:tensor([1024,    7,    5,    3,    1,    0,    4,    2])\n",
      "t_reshape:\n",
      "tensor([[[1024,    7],\n",
      "         [   5,    3]],\n",
      "\n",
      "        [[   1,    0],\n",
      "         [   4,    2]]])\n",
      "t.data 内存地址:2161625247176\n",
      "t_reshape.data 内存地址:2161625247176\n"
     ]
    }
   ],
   "source": [
    "# torch.reshape\n",
    "# randperm是随机排列的一个函数\n",
    "t = torch.randperm(8)\n",
    "print(t)\n",
    "\n",
    "# -1的话就是根据后面那两个参数，计算出-1这个值，然后再转\n",
    "t_reshape = torch.reshape(t, (-1, 2, 2))\n",
    "print(\"t:{}\\nt_reshape:\\n{}\".format(t, t_reshape))\n",
    "\n",
    "t[0] = 1024\n",
    "print(\"t:{}\\nt_reshape:\\n{}\".format(t, t_reshape))\n",
    "print(\"t.data 内存地址:{}\".format(id(t.data)))\n",
    "print(\"t_reshape.data 内存地址:{}\".format(id(t_reshape.data))) # 这个注意一下，两个是共内存的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T08:56:20.803463Z",
     "start_time": "2020-09-01T08:56:20.792522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2111, 0.3209, 0.1134, 0.9590],\n",
      "         [0.6274, 0.5505, 0.8222, 0.9824],\n",
      "         [0.8069, 0.2992, 0.9103, 0.3692]],\n",
      "\n",
      "        [[0.4717, 0.4046, 0.9561, 0.3337],\n",
      "         [0.0794, 0.5144, 0.0961, 0.8525],\n",
      "         [0.9377, 0.7446, 0.2841, 0.0603]]])\n",
      "t shape:torch.Size([2, 3, 4])\n",
      "t_transpose shape：torch.Size([4, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# torch.transpose\n",
    "t = torch.rand((2, 3, 4))      # 产生0-1之间的随机数\n",
    "print(t)\n",
    "t_transpose = torch.transpose(t, dim0=0, dim1=2)    # c*h*w     h*w*c， 这表示第0维和第2维进行交换\n",
    "print(\"t shape:{}\\nt_transpose shape：{}\".format(t.shape, t_transpose.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T08:56:20.812433Z",
     "start_time": "2020-09-01T08:56:20.804455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 1])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3, 1])\n",
      "torch.Size([1, 2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "# torch.squeeze\n",
    "t = torch.rand((1, 2, 3, 1))\n",
    "t_sq = torch.squeeze(t)\n",
    "t_0 = torch.squeeze(t, dim=0)\n",
    "t_1 = torch.squeeze(t, dim=1)\n",
    "print(t.shape)        # torch.Size([1, 2, 3, 1])\n",
    "print(t_sq.shape)     # torch.Size([2, 3])\n",
    "print(t_0.shape)     # torch.Size([2, 3, 1])\n",
    "print(t_1.shape)     # torch.Size([1, 2, 3, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量的数学运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T08:56:20.823406Z",
     "start_time": "2020-09-01T08:56:20.813430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_0:\n",
      "tensor([[-1.5102, -0.3885,  1.1468],\n",
      "        [-1.0240, -0.9455,  0.5452],\n",
      "        [ 1.6457, -0.2422, -0.7472]])\n",
      "t_1:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "t_add_10:\n",
      "tensor([[ 8.4898,  9.6115, 11.1468],\n",
      "        [ 8.9760,  9.0545, 10.5452],\n",
      "        [11.6457,  9.7578,  9.2528]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\torch\\csrc\\utils\\python_arg_parser.cpp:756: UserWarning: This overload of add is deprecated:\n",
      "\tadd(Tensor input, Number alpha, Tensor other, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd(Tensor input, Tensor other, *, Number alpha, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "t_0 = torch.randn((3, 3))\n",
    "t_1 = torch.ones_like(t_0)\n",
    "t_add = torch.add(t_0, 10, t_1)\n",
    "\n",
    "print(\"t_0:\\n{}\\nt_1:\\n{}\\nt_add_10:\\n{}\".format(t_0, t_1, t_add))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性回归是分析一个变量与另外一(多)个变量之间关系的方法。因变量是$y$，自变量是$x$，关系线性：\n",
    "\n",
    "$$y=w\\times x + b$$\n",
    "\n",
    "任务就是求解 $w, b$。\n",
    "\n",
    "我们的求解步骤：\n",
    "\n",
    "1. 确定模型：Model -> $y = w\\times x + b$\n",
    "2. 选择损失函数：这里用 MSE ：$1/m\\sum_{i=1}^{m}(y_i-\\hat{y_i})^2$\n",
    "3. 求解梯度并更新 w, b：\n",
    "$$w=w-lr\\times w\\cdot grad$$\n",
    "$$b=b-lr\\times b\\cdot grad$$\n",
    "\n",
    "这就是我上面说的叫做代码逻辑的一种思路，写代码往往习惯先有一个这样的一种思路，然后再去写代码的时候，就比较容易了。而如果不系统的学一遍 Pytorch，一上来直接上那种复杂的 CNN， LSTM 这种，往往这些代码逻辑不好形成，因为好多细节我们根本就不知道。所以这次学习先从最简单的线性回归开始，然后慢慢的到复杂的那种网络。下面我们开始写一个线性回归模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T08:56:20.860305Z",
     "start_time": "2020-09-01T08:56:20.825398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.1341]) tensor([4.2123])\n"
     ]
    }
   ],
   "source": [
    "# 首先我们得有训练样本X，Y， 这里我们随机生成\n",
    "x = torch.rand(20, 1) * 10\n",
    "y = 2 * x + (5 + torch.randn(20, 1))\n",
    "\n",
    "lr = 0.05\n",
    "\n",
    "# 构建线性回归函数的参数\n",
    "w = torch.randn((1), requires_grad=True)\n",
    "b = torch.zeros((1), requires_grad=True)   # 这俩都需要求梯度\n",
    "\n",
    "for iteration in range(100):\n",
    "    # 前向传播\n",
    "    wx = torch.mul(w, x)\n",
    "    y_pred = torch.add(wx, b)\n",
    "    \n",
    "    # 计算loss\n",
    "    loss = (0.5 * (y - y_pred) ** 2).mean()\n",
    "     \n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "     \n",
    "    # 更新参数\n",
    "    b.data.sub_(lr * b.grad)    # 这种_的加法操作时从自身减，相当于-=\n",
    "    w.data.sub_(lr * w.grad)\n",
    "    \n",
    "    # 梯度清零\n",
    "    w.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "\n",
    "print(w.data, b.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结\n",
    "\n",
    "- 首先我们从 Pytorch 最基本的数据结构开始，认识了张量到底是个什么东西，说白了就是个多维数组，而张量本身有很多的属性，有关于数据本身的 data，dtype，shape，dtype，也有关于求导的 requires_grad，grad，grad_fn，is_leaf；\n",
    "\n",
    "- 然后我们学习了张量的创建方法，比如直接创建，从数组创建，数值创建，按照概率创建等。这里面涉及到了很多的创建函数 tensor()，from_numpy()，ones()，zeros()，eye()，full()，arange()，linspace()，normal()，randn()，rand()，randint()，randperm() 等；\n",
    "\n",
    "- 接着就是张量的操作部分，有基本操作和数学运算，基本操作部分有张量的拼接两个函数 (.cat, .stack)，张量的切分两个函数 (.chunk, .split)，张量的转置 (.reshape, .transpose, .t)，张量的索引两个函数 (.index_select， .masked_select)。数学运算部分，也是很多数学函数，有加减乘除的，指数底数幂函数的，三角函数的很多；\n",
    "\n",
    "- 最后基于上面的所学完成了一个简单的线性回归。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
